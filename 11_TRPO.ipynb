{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0fdba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661edb6",
   "metadata": {},
   "source": [
    "# TRPO (Discrete Action Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7957e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRPO:\n",
    "    \"\"\" TRPO algorithm \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_dim,\n",
    "                 state_space,\n",
    "                 action_space,\n",
    "                 lmbda,\n",
    "                 kl_constraint,\n",
    "                 alpha,\n",
    "                 critic_lr,\n",
    "                 gamma,\n",
    "                 device):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda # GAE parameter\n",
    "        self.kl_constraint = kl_constraint # KL divergence distance\n",
    "        self.alpha = alpha # line search parameter\n",
    "        self.device = device\n",
    "\n",
    "        state_dim = state_space.shape[0]\n",
    "        action_dim = action_space.n\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device) # policy-net don't use torch.optim\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        action_dist = torch.distributions.Categorical(self.actor(state))\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def hessian_matrix_vector_product(self, states, old_action_dists, vector):\n",
    "        \"\"\" Compute Hessian matrix-vector product \"\"\"\n",
    "        new_action_dists = torch.distributions.Categorical(self.actor(states))\n",
    "        kl = torch.mean(torch.distributions.kl.kl_divergence(old_action_dists, new_action_dists)) # compute KL divergence\n",
    "        kl_grad = torch.autograd.grad(kl, self.actor.parameters(), create_graph=True)\n",
    "        kl_grad_vector= torch.cat(grad.view(-1) for grad in kl_grad)\n",
    "        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n",
    "        grad2 = torch.autograd.grad(kl_grad_vector_product, self.actor.parameters())\n",
    "        grad2_vector = torch.cat([grad.view(-1) for grad in grad2])\n",
    "        return grad2_vector\n",
    "\n",
    "    def conjugate_gradient(self, grad, states, old_action_dists):\n",
    "        \"\"\" Solve the equation Hx = g using conjugate gradient \"\"\"\n",
    "        x = torch.zeros_like(grad)\n",
    "        r = grad.clone()\n",
    "        p = grad.clone()\n",
    "        rdotr = torch.dot(r, r)\n",
    "        for i in range(10):\n",
    "            Hp = self.hessian_matrix_vector_product(states, old_action_dists, p)\n",
    "            alpha = rdotr / torch.dot(p, Hp)\n",
    "            x += alpha * p\n",
    "            r -= alpha * Hp\n",
    "            new_rdotr = torch.dot(r, r)\n",
    "            if new_rdotr < 1e-10:\n",
    "                break\n",
    "            beta = new_rdotr / rdotr\n",
    "            p = r + beta * p\n",
    "            rdotr = new_rdotr\n",
    "        return x\n",
    "    \n",
    "    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs, actor):\n",
    "        \"\"\" Compute Policy Objective \"\"\"\n",
    "        log_probs = torch.log(actor(states).gather(1, actions))\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        return torch.mean(ratio * advantage)\n",
    "\n",
    "    def line_search(self, states, actions, advantage, old_log_probs, old_action_dists, max_vec):\n",
    "        \"\"\" Line search to find the step size \"\"\"\n",
    "        old_para = nn.utils.convert_parameters.parameters_to_vector(self.actor.parameters())\n",
    "        old_obj = self.compute_surrogate_obj(states, actions, advantage, old_log_probs, self.actor)\n",
    "        \n",
    "        for i in range(15):\n",
    "            coef = self.alpha ** i\n",
    "            new_para = old_para + coef * max_vec\n",
    "            new_actor = copy.deepcopy(self.actor)\n",
    "            nn.utils.convert_parameters.vector_to_parameters(new_para, new_actor.parameters())\n",
    "            new_action_dists = torch.distributions.Categorical(new_actor(states))\n",
    "            kl_div = torch.mean(torch.distributions.kl.kl_divergence(old_action_dists, new_action_dists))\n",
    "            new_obj = self.compute_surrogate_obj(states, actions, advantage, old_log_probs, new_actor)\n",
    "            if new_obj > old_obj and kl_div < self.kl_constraint:\n",
    "                return new_para\n",
    "        return old_para\n",
    "    \n",
    "    def policy_learn(self, states, actions, old_action_dists, old_log_probs, advantage):\n",
    "        \"\"\" Update Policy Function \"\"\"\n",
    "        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage, old_log_probs, self.actor)\n",
    "        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
